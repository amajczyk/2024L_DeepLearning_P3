{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/uygarkurt/DDPM-Image-Generation/blob/main/DDPM_Image_Generartion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import SGD\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random \n",
    "import timeit\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 128\n",
    "DATASET_PERCENT = 0.1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 30\n",
    "NUM_GENERATE_IMAGES = 9\n",
    "NUM_TIMESTEPS = 1000\n",
    "MIXED_PRECISION = \"fp16\"\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5beef211c4a40309b24733aa921b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_dataset_path = f\"data/square{IMG_SIZE}_random{str(DATASET_PERCENT)}/\"\n",
    "dataset = load_dataset('imagefolder', data_dir=local_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "[\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_small = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=1,\n",
    "    block_out_channels=(64, 64, 128, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "model_small = model_small.to(device)\n",
    "\n",
    "model_mid = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "model_mid = model_mid.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model_big = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=3,\n",
    "    block_out_channels=(128, 256, 256, 512, 512, 1024, 1024),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "model_big = model_big.to(device)\n",
    "\n",
    "\n",
    "models = [model_small, model_mid, model_big]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image_generation(model, noise_scheduler, num_generate_images, random_seed, num_timesteps):\n",
    "    pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "    \n",
    "    images = pipeline(\n",
    "        batch_size=num_generate_images,\n",
    "        generator=torch.manual_seed(random_seed),\n",
    "        num_inference_steps=num_timesteps\n",
    "    ).images\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    for i in range(1, num_generate_images+1):\n",
    "        fig.add_subplot(3, 3, i)\n",
    "        plt.imshow(images[i-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"Calculate the FrÃ©chet Distance between two multivariate Gaussians.\"\"\"\n",
    "    covmean, _ = sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    return diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "def get_activations(model, dataloader, device, key):\n",
    "    \"\"\"Get activations of the dataset images using the InceptionV3 model.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Calculating activations\", leave=False):\n",
    "        images = batch[key].to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "        activations.append(preds.cpu().numpy())\n",
    "\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "def calculate_statistics(activations):\n",
    "    \"\"\"Calculate mean and covariance matrix of activations.\"\"\"\n",
    "    mu = np.mean(activations, axis=0)\n",
    "    sigma = np.cov(activations, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def sample_image_generation(model, noise_scheduler, num_generate_images, random_seed, num_timesteps, device):\n",
    "    pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "    generator = torch.manual_seed(random_seed)\n",
    "    \n",
    "    images = pipeline(\n",
    "        batch_size=num_generate_images,\n",
    "        generator=generator,\n",
    "        num_inference_steps=num_timesteps\n",
    "    ).images\n",
    "    \n",
    "    # Transform images to tensor and normalize\n",
    "    transform = preprocess\n",
    "    \n",
    "    transformed_images = torch.stack([transform(image) for image in images]).to(device)\n",
    "    return transformed_images\n",
    "\n",
    "def calculate_fid(model, dataloader_real, num_generated_images, noise_scheduler, random_seed, num_timesteps, device):\n",
    "    \"\"\"Calculate FID score for real and generated images.\"\"\"\n",
    "    # Load InceptionV3 model\n",
    "    inception = inception_v3(pretrained=True, transform_input=False)\n",
    "    inception.fc = nn.Identity()  # Remove the last fully connected layer\n",
    "    inception.to(device)\n",
    "\n",
    "    # Get activations for real images\n",
    "    real_activations = get_activations(inception, dataloader_real, device, key = 'images')\n",
    "\n",
    "    # Generate images and get activations for generated images\n",
    "    generated_images = sample_image_generation(model, noise_scheduler, num_generated_images, random_seed, num_timesteps, device)\n",
    "    generated_dataset = TensorDataset(generated_images)\n",
    "    generated_dataloader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n",
    "    generated_activations = get_activations(inception, generated_dataloader, device, key = 0)\n",
    "\n",
    "    # Calculate statistics\n",
    "    mu_real, sigma_real = calculate_statistics(real_activations)\n",
    "    mu_generated, sigma_generated = calculate_statistics(generated_activations)\n",
    "\n",
    "    # Calculate FID\n",
    "    fid_score = calculate_frechet_distance(mu_real, sigma_real, mu_generated, sigma_generated)\n",
    "    return fid_score\n",
    "\n",
    "\n",
    "transform = preprocess\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c794ddd75348619b6513d42121f922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCHS:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce04e1f70e6e41cab2c7185af6045cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/1895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_GENERATE_IMAGES_FID = len(train_dataloader)  \n",
    "\n",
    "for model in models:\n",
    "    for learning_rate in [LEARNING_RATE / 10, LEARNING_RATE, LEARNING_RATE * 10]:\n",
    "        for optimizer in [\"Adam\", \"SGD\"]:\n",
    "            if optimizer == \"Adam\":\n",
    "                optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "            elif optimizer == \"SGD\":\n",
    "                optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            training_loss = []\n",
    "            frechet_inception_distance = []\n",
    "\n",
    "\n",
    "            lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "\n",
    "                optimizer=optimizer,\n",
    "\n",
    "                num_warmup_steps=500,\n",
    "\n",
    "                num_training_steps=len(train_dataloader) * NUM_EPOCHS,\n",
    "            )\n",
    "\n",
    "\n",
    "            accelerator = Accelerator(\n",
    "\n",
    "                mixed_precision=MIXED_PRECISION,\n",
    "\n",
    "                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            )\n",
    "\n",
    "\n",
    "            model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "                model, optimizer, train_dataloader, lr_scheduler\n",
    "            )\n",
    "\n",
    "\n",
    "            start = timeit.default_timer()\n",
    "\n",
    "            for epoch in tqdm(range(NUM_EPOCHS), position=0, leave=True, desc=\"EPOCHS\"):\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                train_running_loss = 0\n",
    "\n",
    "                for idx, batch in enumerate(\n",
    "                    tqdm(train_dataloader, position=0, desc=\"BATCHES\", leave=False)\n",
    "                ):\n",
    "\n",
    "                    clean_images = batch[\"images\"].to(device)\n",
    "\n",
    "                    noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "                    last_batch_size = len(clean_images)\n",
    "\n",
    "\n",
    "                    timesteps = torch.randint(\n",
    "                        0,\n",
    "                        noise_scheduler.config.num_train_timesteps,\n",
    "                        (last_batch_size,),\n",
    "                    ).to(device)\n",
    "\n",
    "                    noisy_images = noise_scheduler.add_noise(\n",
    "                        clean_images, noise, timesteps\n",
    "                    )\n",
    "\n",
    "\n",
    "                    with accelerator.accumulate(model):\n",
    "\n",
    "                        noise_pred = model(noisy_images, timesteps, return_dict=False)[\n",
    "                            0\n",
    "                        ]\n",
    "                        loss = F.mse_loss(noise_pred, noise)\n",
    "                        accelerator.backward(loss)\n",
    "\n",
    "                        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    train_running_loss += loss.item()\n",
    "                train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "                training_loss.append(train_loss)\n",
    "                fid_score = calculate_fid(\n",
    "                    model,\n",
    "                    train_dataloader,\n",
    "                    NUM_GENERATE_IMAGES_FID,\n",
    "                    noise_scheduler,\n",
    "                    RANDOM_SEED,\n",
    "                    NUM_TIMESTEPS,\n",
    "                    device,\n",
    "                )\n",
    "\n",
    "                frechet_inception_distance.append(fid_score)\n",
    "\n",
    "                train_learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "                print(f\"Train Loss EPOCH: {epoch+1}: {train_loss:.4f}\")\n",
    "\n",
    "                print(f\"Train Learning Rate EPOCH: {epoch+1}: {train_learning_rate}\")\n",
    "\n",
    "                print(f\"FID Score EPOCH: {epoch+1}: {fid_score:.4f}\")\n",
    "\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "            stop = timeit.default_timer()\n",
    "\n",
    "            print(f\"Training Time: {stop-start:.2f}s\")\n",
    "\n",
    "            # save model with date and time in a folder\n",
    "            os.makedirs(\"models\", exist_ok=True)\n",
    "            time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            model_path = f\"models/{time}\"\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{model_path}/model.pth\")\n",
    "\n",
    "            # save optimizer\n",
    "            torch.save(optimizer.state_dict(), f\"{model_path}/optimizer.pth\")\n",
    "\n",
    "            # save lr_scheduler\n",
    "            torch.save(lr_scheduler.state_dict(), f\"{model_path}/lr_scheduler.pth\")\n",
    "\n",
    "            # save noise_scheduler\n",
    "            torch.save(noise_scheduler, f\"{model_path}/noise_scheduler.pth\")\n",
    "\n",
    "            # save metadata\n",
    "            metadata = {\n",
    "                \"IMG_SIZE\": IMG_SIZE,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                \"LEARNING_RATE\": LEARNING_RATE,\n",
    "                \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "                \"NUM_GENERATE_IMAGES\": NUM_GENERATE_IMAGES,\n",
    "                \"NUM_TIMESTEPS\": NUM_TIMESTEPS,\n",
    "                \"MIXED_PRECISION\": MIXED_PRECISION,\n",
    "                \"GRADIENT_ACCUMULATION_STEPS\": GRADIENT_ACCUMULATION_STEPS,\n",
    "                \"losses\": training_loss,\n",
    "                \"fid_scores\": frechet_inception_distance,\n",
    "                \"dataset\": f\"square{IMG_SIZE}_random{str(DATASET_PERCENT)}\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/uygarkurt/DDPM-Image-Generation/blob/main/DDPM_Image_Generartion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
    "\n",
    "\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import json\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "# ignore UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 96\n",
    "DATASET_PERCENTAGE = 0.005\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 35\n",
    "NUM_GENERATE_IMAGES = 9\n",
    "NUM_TIMESTEPS = 500\n",
    "MIXED_PRECISION = \"fp16\"\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b7cce8a8ed4ffeaacedc38fc0ba90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_dataset_path = f\"data/square{IMG_SIZE}_random{str(DATASET_PERCENTAGE)}/\"\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=local_dataset_path)\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=1,\n",
    "    block_out_channels=(64, 64, 128, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model_small = model_small.to(device)\n",
    "\n",
    "model_mid = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model_mid = model_mid.to(device)\n",
    "\n",
    "\n",
    "model_big = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=3,\n",
    "    block_out_channels=(128, 256, 256, 512, 512, 1024, 1024),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model_big = model_big.to(device)\n",
    "models = [model_small, model_mid, model_big]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"Calculate the FrÃ©chet Distance between two multivariate Gaussians.\"\"\"\n",
    "    covmean, _ = sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    return diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "\n",
    "def get_activations(model, dataloader, device, key):\n",
    "    \"\"\"Get activations of the dataset images using the InceptionV3 model.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Calculating activations\", leave=False):\n",
    "        images = batch[key].to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "        activations.append(preds.cpu().numpy())\n",
    "\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def calculate_statistics(activations):\n",
    "    \"\"\"Calculate mean and covariance matrix of activations.\"\"\"\n",
    "    mu = np.mean(activations, axis=0)\n",
    "    sigma = np.cov(activations, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def sample_image_generation(\n",
    "    model,\n",
    "    noise_scheduler,\n",
    "    num_generate_images,\n",
    "    random_seed,\n",
    "    num_timesteps,\n",
    "    device,\n",
    "    accelerator,\n",
    "):\n",
    "    pipeline = DDPMPipeline(\n",
    "        unet=accelerator.unwrap_model(model), scheduler=noise_scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "    images = pipeline(\n",
    "        batch_size=num_generate_images,\n",
    "        generator=torch.manual_seed(random_seed),\n",
    "        num_inference_steps=num_timesteps,\n",
    "       \n",
    "    ).images\n",
    " \n",
    "    \n",
    "\n",
    "    # Transform images to tensor and normalize\n",
    "    transform = preprocess\n",
    "\n",
    "    transformed_images = torch.stack([transform(image) for image in images]).to(device)\n",
    "    return transformed_images\n",
    "\n",
    "\n",
    "def calculate_fid(\n",
    "    model,\n",
    "    dataloader_real,\n",
    "    num_generated_images,\n",
    "    noise_scheduler,\n",
    "    random_seed,\n",
    "    num_timesteps,\n",
    "    device,\n",
    "    accelerator,\n",
    "):\n",
    "    \"\"\"Calculate FID score for real and generated images.\"\"\"\n",
    "    # Load InceptionV3 model\n",
    "    inception = inception_v3(pretrained=True, transform_input=False)\n",
    "    inception.fc = nn.Identity()  # Remove the last fully connected layer\n",
    "    inception.to(device)\n",
    "\n",
    "    # Get activations for real images\n",
    "    real_activations = get_activations(inception, dataloader_real, device, key=\"images\")\n",
    "\n",
    "    # Generate images and get activations for generated images\n",
    "    generated_images = sample_image_generation(\n",
    "        model,\n",
    "        noise_scheduler,\n",
    "        num_generated_images,\n",
    "        random_seed,\n",
    "        num_timesteps,\n",
    "        device,\n",
    "        accelerator,\n",
    "    )\n",
    "    \n",
    "    generated_dataset = TensorDataset(generated_images)\n",
    "    generated_dataloader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n",
    "    generated_activations = get_activations(\n",
    "        inception, generated_dataloader, device, key=0\n",
    "    )\n",
    "\n",
    "    # Calculate statistics\n",
    "    mu_real, sigma_real = calculate_statistics(real_activations)\n",
    "    mu_generated, sigma_generated = calculate_statistics(generated_activations)\n",
    "\n",
    "    # Calculate FID\n",
    "    fid_score = calculate_frechet_distance(\n",
    "        mu_real, sigma_real, mu_generated, sigma_generated\n",
    "    )\n",
    "    return fid_score\n",
    "\n",
    "\n",
    "transform = preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET Size: mid, Learning Rate: 0.001, Optimizer: Adam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdeba22500340f2a4e93ffea36e8cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCHS:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572521696a1c4538ae368b2eca55334d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a0ecc8f8964e30b876fd743e60b1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating activations:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e5365a1adc4bcd9f21429b2b17f54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2e977cfa394113b3655fcbac30294b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating activations:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH: 1: 0.2766\n",
      "Train Learning Rate EPOCH: 1: 0.00019\n",
      "FID Score EPOCH: 1: 275.7989\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16c23e88a854a70830fba6ca81e5869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH: 2: 0.0618\n",
      "Train Learning Rate EPOCH: 2: 0.00038\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39715aca812a412591c71006972d8ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH: 3: nan\n",
      "Train Learning Rate EPOCH: 3: 0.0005620000000000001\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab868f90bf4a799c6da90c8ff7f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH: 4: nan\n",
      "Train Learning Rate EPOCH: 4: 0.0005620000000000001\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20476dd59b446f496ce283da9ebb64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH: 5: nan\n",
      "Train Learning Rate EPOCH: 5: 0.0005620000000000001\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0a55d1dc1e406b9bf97b224e475e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d2730696e3473aa3714c5fd23dea94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating activations:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb1a0b01b2f48f9a838c5a2ec61b13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n_seeds in range(5):\n",
    "    for model, unet_size in zip(models, [\n",
    "        \"small\", \n",
    "        \"mid\", \n",
    "        \"big\"\n",
    "        ]):\n",
    "        for learning_rate in [\n",
    "            LEARNING_RATE / 10, \n",
    "            LEARNING_RATE, \n",
    "            LEARNING_RATE * 10\n",
    "            ]:\n",
    "            for optimizer_type in [\n",
    "                \"Adam\", \n",
    "                # \"SGD\"\n",
    "                ]:\n",
    "                loss_is_nan = False\n",
    "\n",
    "                if unet_size == \"small\" and learning_rate and n_seeds == 0:\n",
    "                    continue\n",
    "                if unet_size == \"mid\" and learning_rate != LEARNING_RATE * 10 and n_seeds == 0:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                print(f\"UNET Size: {unet_size}, Learning Rate: {learning_rate}, Optimizer: {optimizer_type}\")\n",
    "\n",
    "\n",
    "                if optimizer_type == \"Adam\":\n",
    "                    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "                elif optimizer_type == \"SGD\":\n",
    "                    optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                training_loss = []\n",
    "                frechet_inception_distance = []\n",
    "                frechet_inception_distance_epochs = []\n",
    "\n",
    "                lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer=optimizer,\n",
    "                    num_warmup_steps=500,\n",
    "                    num_training_steps=len(train_dataloader) * NUM_EPOCHS,\n",
    "                )\n",
    "                \n",
    "                noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TIMESTEPS)\n",
    "\n",
    "                accelerator = Accelerator(\n",
    "                    mixed_precision=MIXED_PRECISION,\n",
    "                    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "                )\n",
    "\n",
    "                model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "                    model, optimizer, train_dataloader, lr_scheduler\n",
    "                )\n",
    "\n",
    "                start = timeit.default_timer()\n",
    "\n",
    "                for epoch in tqdm(range(NUM_EPOCHS), position=0, leave=True, desc=\"EPOCHS\"):\n",
    "\n",
    "                    def seed_worker(worker_id):\n",
    "                        worker_seed = torch.initial_seed() % 2**32\n",
    "                        np.random.seed(worker_seed)\n",
    "                        random.seed(worker_seed)\n",
    "\n",
    "                    g = torch.Generator()\n",
    "                    g.manual_seed(n_seeds)\n",
    "\n",
    "                    train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                    train_running_loss = 0\n",
    "\n",
    "                    for idx, batch in enumerate(\n",
    "                        tqdm(train_dataloader, position=0, desc=\"BATCHES\", leave=False)\n",
    "                    ):\n",
    "\n",
    "                        clean_images = batch[\"images\"].to(device)\n",
    "\n",
    "                        noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "                        last_batch_size = len(clean_images)\n",
    "\n",
    "                        timesteps = torch.randint(\n",
    "                            0,\n",
    "                            noise_scheduler.config.num_train_timesteps,\n",
    "                            (last_batch_size,),\n",
    "                        ).to(device)\n",
    "\n",
    "                        noisy_images = noise_scheduler.add_noise(\n",
    "                            clean_images, noise, timesteps\n",
    "                        )\n",
    "\n",
    "                        with accelerator.accumulate(model):\n",
    "\n",
    "                            noise_pred = model(noisy_images, timesteps, return_dict=False)[\n",
    "                                0\n",
    "                            ]\n",
    "                            loss = F.mse_loss(noise_pred, noise)\n",
    "                            accelerator.backward(loss)\n",
    "\n",
    "                            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                            optimizer.step()\n",
    "                            lr_scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        train_running_loss += loss.item()\n",
    "                    train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "                    \n",
    "\n",
    "                    training_loss.append(train_loss)\n",
    "                    if epoch % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "                        fid_score = calculate_fid(\n",
    "                            model,\n",
    "                            train_dataloader,\n",
    "                            len(train_dataloader),\n",
    "                            noise_scheduler,\n",
    "                            RANDOM_SEED,\n",
    "                            NUM_TIMESTEPS,\n",
    "                            device,\n",
    "                            accelerator,\n",
    "                        )\n",
    "\n",
    "                        frechet_inception_distance.append(fid_score)\n",
    "                        frechet_inception_distance_epochs.append(epoch)\n",
    "\n",
    "                    train_learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "                    print(f\"Train Loss EPOCH: {epoch+1}: {train_loss:.4f}\")\n",
    "\n",
    "                    print(f\"Train Learning Rate EPOCH: {epoch+1}: {train_learning_rate}\")\n",
    "\n",
    "                    if epoch % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "                        print(f\"FID Score EPOCH: {epoch+1}: {fid_score:.4f}\")\n",
    "\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "                    # if train_loss is not a number, break\n",
    "                    if train_loss != train_loss:\n",
    "                        print(\"Loss is NaN. Exiting training.\")\n",
    "                        loss_is_nan = True\n",
    "                        break\n",
    "\n",
    "                stop = timeit.default_timer()\n",
    "\n",
    "                print(f\"Training Time: {stop-start:.2f}s\")\n",
    "\n",
    "                # save model with date and time in a folder\n",
    "                os.makedirs(\"models\", exist_ok=True)\n",
    "                time_ = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "                model_path = f\"models/ddpm/hypers/{time_}-{unet_size}-{optimizer_type}-{str(learning_rate)}-{n_seeds}-{DATASET_PERCENTAGE}-{IMG_SIZE}\"\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "                \n",
    "              \n",
    "\n",
    "                torch.save(model.state_dict(), f\"{model_path}/model.pth\")\n",
    "                torch.save(optimizer.state_dict(), f\"{model_path}/optimizer.pth\")\n",
    "                torch.save(lr_scheduler.state_dict(), f\"{model_path}/lr_scheduler.pth\")\n",
    "                torch.save(noise_scheduler, f\"{model_path}/noise_scheduler.pth\")\n",
    "\n",
    "                metadata = {\n",
    "                    \"IMG_SIZE\": IMG_SIZE,\n",
    "                    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "                    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "                    \"NUM_GENERATE_IMAGES\": NUM_GENERATE_IMAGES,\n",
    "                    \"NUM_TIMESTEPS\": NUM_TIMESTEPS,\n",
    "                    \"MIXED_PRECISION\": MIXED_PRECISION,\n",
    "                    \"GRADIENT_ACCUMULATION_STEPS\": GRADIENT_ACCUMULATION_STEPS,\n",
    "                    \"losses\": training_loss,\n",
    "                    \"fid_scores\": frechet_inception_distance,\n",
    "                    \"fid_scores_epochs\": frechet_inception_distance_epochs,\n",
    "                    \"dataset\": f\"square{IMG_SIZE}_random{str(DATASET_PERCENTAGE)}\",\n",
    "                    \"UNET_size\": unet_size,\n",
    "                    \"optimizer\": optimizer_type,\n",
    "                    'seed': n_seeds,\n",
    "                    'exited': loss_is_nan,\n",
    "                    'execution_time': stop-start,\n",
    "\n",
    "\n",
    "                }\n",
    "\n",
    "                with open(f\"{model_path}/metadata.json\", \"w\") as f:\n",
    "                    json.dump(metadata, f)\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

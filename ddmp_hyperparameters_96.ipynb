{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/uygarkurt/DDPM-Image-Generation/blob/main/DDPM_Image_Generartion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
    "\n",
    "\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import json\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "# ignore UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 96\n",
    "DATASET_PERCENTAGE = 0.001\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 35\n",
    "NUM_GENERATE_IMAGES = 9\n",
    "NUM_TIMESTEPS = 500\n",
    "MIXED_PRECISION = \"fp16\"\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_path = f\"data/square{IMG_SIZE}_random{str(DATASET_PERCENTAGE)}/\"\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=local_dataset_path)\n",
    "dataset = dataset[\"train\"]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=1,\n",
    "    block_out_channels=(64, 64, 128, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model_small = model_small.to(device)\n",
    "\n",
    "model_mid = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model_mid = model_mid.to(device)\n",
    "\n",
    "\n",
    "model_big = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=3,\n",
    "    block_out_channels=(128, 256, 256, 512, 512, 768),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"Calculate the Fr√©chet Distance between two multivariate Gaussians.\"\"\"\n",
    "    covmean, _ = sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    return diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "\n",
    "def get_activations(model, dataloader, device, key):\n",
    "    \"\"\"Get activations of the dataset images using the InceptionV3 model.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Calculating activations\", leave=False):\n",
    "        images = batch[key].to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)\n",
    "        activations.append(preds.cpu().numpy())\n",
    "\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def calculate_statistics(activations):\n",
    "    \"\"\"Calculate mean and covariance matrix of activations.\"\"\"\n",
    "    mu = np.mean(activations, axis=0)\n",
    "    sigma = np.cov(activations, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def sample_image_generation(\n",
    "    model,\n",
    "    noise_scheduler,\n",
    "    num_generate_images,\n",
    "    random_seed,\n",
    "    num_timesteps,\n",
    "    device,\n",
    "    accelerator,\n",
    "):\n",
    "    pipeline = DDPMPipeline(\n",
    "        unet=accelerator.unwrap_model(model), scheduler=noise_scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "    images = pipeline(\n",
    "        batch_size=num_generate_images,\n",
    "        generator=torch.manual_seed(random_seed),\n",
    "        num_inference_steps=num_timesteps,\n",
    "       \n",
    "    ).images\n",
    " \n",
    "    \n",
    "\n",
    "    # Transform images to tensor and normalize\n",
    "    transform = preprocess\n",
    "\n",
    "    transformed_images = torch.stack([transform(image) for image in images]).to(device)\n",
    "    return transformed_images\n",
    "\n",
    "\n",
    "def calculate_fid(\n",
    "    model,\n",
    "    dataloader_real,\n",
    "    num_generated_images,\n",
    "    noise_scheduler,\n",
    "    random_seed,\n",
    "    num_timesteps,\n",
    "    device,\n",
    "    accelerator,\n",
    "):\n",
    "    \"\"\"Calculate FID score for real and generated images.\"\"\"\n",
    "    # Load InceptionV3 model\n",
    "    inception = inception_v3(pretrained=True, transform_input=False)\n",
    "    inception.fc = nn.Identity()  # Remove the last fully connected layer\n",
    "    inception.to(device)\n",
    "\n",
    "    # Get activations for real images\n",
    "    real_activations = get_activations(inception, dataloader_real, device, key=\"images\")\n",
    "\n",
    "    # Generate images and get activations for generated images\n",
    "    generated_images = sample_image_generation(\n",
    "        model,\n",
    "        noise_scheduler,\n",
    "        num_generated_images,\n",
    "        random_seed,\n",
    "        num_timesteps,\n",
    "        device,\n",
    "        accelerator,\n",
    "    )\n",
    "    \n",
    "    generated_dataset = TensorDataset(generated_images)\n",
    "    generated_dataloader = DataLoader(generated_dataset, batch_size=32, shuffle=False)\n",
    "    generated_activations = get_activations(\n",
    "        inception, generated_dataloader, device, key=0\n",
    "    )\n",
    "\n",
    "    # Calculate statistics\n",
    "    mu_real, sigma_real = calculate_statistics(real_activations)\n",
    "    mu_generated, sigma_generated = calculate_statistics(generated_activations)\n",
    "\n",
    "    # Calculate FID\n",
    "    fid_score = calculate_frechet_distance(\n",
    "        mu_real, sigma_real, mu_generated, sigma_generated\n",
    "    )\n",
    "    return fid_score\n",
    "\n",
    "\n",
    "transform = preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET Size: mid, Learning Rate: 1e-05, Optimizer: SGD\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa3f7eda5eb47a2b4b14ded1e4ba5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCHS:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cae75bfb384aae8c307bd7a3e0cf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 100\u001b[0m\n\u001b[0;32m     96\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m model(noisy_images, timesteps, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     98\u001b[0m ]\n\u001b[0;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[1;32m--> 100\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\adamm\\miniconda3\\envs\\torchenv\\lib\\site-packages\\accelerate\\accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\adamm\\miniconda3\\envs\\torchenv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adamm\\miniconda3\\envs\\torchenv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adamm\\miniconda3\\envs\\torchenv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "for unet_size in ([\n",
    "    \"small\", \n",
    "    \"mid\", \n",
    "    \"big\"\n",
    "    ]):\n",
    "    for learning_rate in [\n",
    "        LEARNING_RATE / 10, \n",
    "        LEARNING_RATE, \n",
    "        LEARNING_RATE * 10\n",
    "        ]:\n",
    "        for optimizer_type in [\n",
    "            # \"Adam\", \n",
    "            \"SGD\"\n",
    "            ]:\n",
    "            for n_seeds in range(5):\n",
    "                loss_is_nan = False\n",
    "\n",
    "                if unet_size == \"small\" and learning_rate == LEARNING_RATE / 10:\n",
    "                    continue\n",
    "\n",
    "                model = deepcopy(model_small) if unet_size == \"small\" else deepcopy(model_mid) if unet_size == \"mid\" else deepcopy(model_big) \n",
    "                \n",
    "                \n",
    "                print(f\"UNET Size: {unet_size}, Learning Rate: {learning_rate}, Optimizer: {optimizer_type}\")\n",
    "\n",
    "\n",
    "                if optimizer_type == \"Adam\":\n",
    "                    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "                elif optimizer_type == \"SGD\":\n",
    "                    optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                training_loss = []\n",
    "                frechet_inception_distance = []\n",
    "                frechet_inception_distance_epochs = []\n",
    "\n",
    "                lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer=optimizer,\n",
    "                    num_warmup_steps=500,\n",
    "                    num_training_steps=len(train_dataloader) * NUM_EPOCHS,\n",
    "                )\n",
    "                \n",
    "                noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TIMESTEPS)\n",
    "\n",
    "                accelerator = Accelerator(\n",
    "                    mixed_precision=MIXED_PRECISION,\n",
    "                    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "                )\n",
    "\n",
    "                model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "                    model, optimizer, train_dataloader, lr_scheduler\n",
    "                )\n",
    "\n",
    "                start = timeit.default_timer()\n",
    "\n",
    "                for epoch in tqdm(range(NUM_EPOCHS), position=0, leave=True, desc=\"EPOCHS\"):\n",
    "\n",
    "                    def seed_worker(worker_id):\n",
    "                        worker_seed = torch.initial_seed() % 2**32\n",
    "                        np.random.seed(worker_seed)\n",
    "                        random.seed(worker_seed)\n",
    "\n",
    "                    g = torch.Generator()\n",
    "                    g.manual_seed(n_seeds)\n",
    "\n",
    "                    train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                    train_running_loss = 0\n",
    "\n",
    "                    for idx, batch in enumerate(\n",
    "                        tqdm(train_dataloader, position=0, desc=\"BATCHES\", leave=False)\n",
    "                    ):\n",
    "\n",
    "                        clean_images = batch[\"images\"].to(device)\n",
    "\n",
    "                        noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "                        last_batch_size = len(clean_images)\n",
    "\n",
    "                        timesteps = torch.randint(\n",
    "                            0,\n",
    "                            noise_scheduler.config.num_train_timesteps,\n",
    "                            (last_batch_size,),\n",
    "                        ).to(device)\n",
    "\n",
    "                        noisy_images = noise_scheduler.add_noise(\n",
    "                            clean_images, noise, timesteps\n",
    "                        )\n",
    "\n",
    "                        with accelerator.accumulate(model):\n",
    "\n",
    "                            noise_pred = model(noisy_images, timesteps, return_dict=False)[\n",
    "                                0\n",
    "                            ]\n",
    "                            loss = F.mse_loss(noise_pred, noise)\n",
    "                            accelerator.backward(loss)\n",
    "\n",
    "                            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                            optimizer.step()\n",
    "                            lr_scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        train_running_loss += loss.item()\n",
    "                    train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "                    \n",
    "\n",
    "                    training_loss.append(train_loss)\n",
    "                    if epoch % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "                        fid_score = calculate_fid(\n",
    "                            model,\n",
    "                            train_dataloader,\n",
    "                            len(train_dataloader),\n",
    "                            noise_scheduler,\n",
    "                            RANDOM_SEED,\n",
    "                            NUM_TIMESTEPS,\n",
    "                            device,\n",
    "                            accelerator,\n",
    "                        )\n",
    "\n",
    "                        frechet_inception_distance.append(fid_score)\n",
    "                        frechet_inception_distance_epochs.append(epoch)\n",
    "\n",
    "                    train_learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "                    print(f\"Train Loss EPOCH: {epoch+1}: {train_loss:.4f}\")\n",
    "\n",
    "                    print(f\"Train Learning Rate EPOCH: {epoch+1}: {train_learning_rate}\")\n",
    "\n",
    "                    if epoch % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "                        print(f\"FID Score EPOCH: {epoch+1}: {fid_score:.4f}\")\n",
    "\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "                    # if train_loss is not a number, break\n",
    "                    if train_loss != train_loss:\n",
    "                        print(\"Loss is NaN. Exiting training.\")\n",
    "                        loss_is_nan = True\n",
    "                        break\n",
    "\n",
    "                stop = timeit.default_timer()\n",
    "\n",
    "                print(f\"Training Time: {stop-start:.2f}s\")\n",
    "\n",
    "                # save model with date and time in a folder\n",
    "                if learning_rate == 1e-5:\n",
    "                    learning_rate_str = \"0.00001\"\n",
    "                else:\n",
    "                    learning_rate_str = str(learning_rate)\n",
    "                os.makedirs(\"models\", exist_ok=True)\n",
    "                time_ = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "                model_path = f\"models/ddpm/hypers/{DATASET_PERCENTAGE}/{time_}-{unet_size}-{optimizer_type}-{learning_rate_str}-{n_seeds}-{DATASET_PERCENTAGE}-{IMG_SIZE}\"\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "                \n",
    "                \n",
    "\n",
    "                torch.save(model.state_dict(), f\"{model_path}/model.pth\")\n",
    "                torch.save(optimizer.state_dict(), f\"{model_path}/optimizer.pth\")\n",
    "                torch.save(lr_scheduler.state_dict(), f\"{model_path}/lr_scheduler.pth\")\n",
    "                torch.save(noise_scheduler, f\"{model_path}/noise_scheduler.pth\")\n",
    "\n",
    "                metadata = {\n",
    "                    \"IMG_SIZE\": IMG_SIZE,\n",
    "                    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "                    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "                    \"NUM_GENERATE_IMAGES\": NUM_GENERATE_IMAGES,\n",
    "                    \"NUM_TIMESTEPS\": NUM_TIMESTEPS,\n",
    "                    \"MIXED_PRECISION\": MIXED_PRECISION,\n",
    "                    \"GRADIENT_ACCUMULATION_STEPS\": GRADIENT_ACCUMULATION_STEPS,\n",
    "                    \"losses\": training_loss,\n",
    "                    \"fid_scores\": frechet_inception_distance,\n",
    "                    \"fid_scores_epochs\": frechet_inception_distance_epochs,\n",
    "                    \"dataset\": f\"square{IMG_SIZE}_random{str(DATASET_PERCENTAGE)}\",\n",
    "                    \"UNET_size\": unet_size,\n",
    "                    \"optimizer\": optimizer_type,\n",
    "                    'seed': n_seeds,\n",
    "                    'exited_loss_na': loss_is_nan,\n",
    "                    'execution_time': stop-start,\n",
    "\n",
    "\n",
    "                }\n",
    "\n",
    "                with open(f\"{model_path}/metadata.json\", \"w\") as f:\n",
    "                    json.dump(metadata, f)\n",
    "\n",
    "                clear_output()\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
